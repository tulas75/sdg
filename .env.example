# LiteLLM Configuration
LITELLM_PROVIDER=ollama_chat
MODEL_NAME=gemma3:4b-it-fp16

# Flask Configuration
FLASK_ENV=development
FLASK_APP=app/main.py

# Q/A Generation Configuration
QA_PER_CHUNK=3
CHUNK_SIZE=2000